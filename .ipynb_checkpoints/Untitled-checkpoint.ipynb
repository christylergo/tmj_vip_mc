{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5c8f9ab5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [120]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    183\u001b[0m doc_rf \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvip_daily_sales\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# 日销量、商品链接\u001b[39;00m\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_words\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m商品明细|条码粒度\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_pos\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m条码\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m日期\u001b[39m\u001b[38;5;124m'\u001b[39m, ], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_pos\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m销售量\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m商品链接\u001b[39m\u001b[38;5;124m'\u001b[39m, ],\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_type\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m'\u001b[39m, ],\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaution\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    188\u001b[0m     }\n\u001b[0;32m    189\u001b[0m get_raw_data \u001b[38;5;241m=\u001b[39m DocumentIO(doc_rf)\n\u001b[1;32m--> 190\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mget_raw_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [120]\u001b[0m, in \u001b[0;36mDocumentIO.get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame():\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_sql \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerge\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 136\u001b[0m         doc_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m         sql_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_sqlite()\n\u001b[0;32m    138\u001b[0m         sql_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "Input \u001b[1;32mIn [120]\u001b[0m, in \u001b[0;36mDocumentIO.read_doc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m doc_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles:\n\u001b[1;32m--> 107\u001b[0m     matched_csv \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m^.*\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m.csv$\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     matched_excel \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^.*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.xlsx?$\u001b[39m\u001b[38;5;124m'\u001b[39m, file)\n\u001b[0;32m    109\u001b[0m     pd_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_ref[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_pos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_ref[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_pos\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\python38\\lib\\re.py:189\u001b[0m, in \u001b[0;36mmatch\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;124;03m\"\"\"Try to apply the pattern at the start of the string, returning\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 461, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 450, in process_one\n",
      "    await dispatch(*args)\n",
      "TypeError: object NoneType can't be used in 'await' expression\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python38\\lib\\logging\\__init__.py\", line 1085, in emit\n",
      "    self.flush()\n",
      "  File \"c:\\python38\\lib\\logging\\__init__.py\", line 1065, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 22] Invalid argument\n",
      "Call stack:\n",
      "  File \"c:\\python38\\lib\\runpy.py\", line 192, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\python38\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\python38\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\python38\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\python38\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\python38\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\python38\\lib\\asyncio\\base_events.py\", line 563, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\python38\\lib\\asyncio\\base_events.py\", line 1844, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\python38\\lib\\asyncio\\events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 463, in dispatch_queue\n",
      "    self.log.exception(\"Error in message handler\")\n",
      "Message: 'Error in message handler'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "import sqlite3 as sqlite\n",
    "\n",
    "import settings as st\n",
    "import sqlite_init\n",
    "\n",
    "\n",
    "# CPUS = os.cpu_count()\n",
    "class DocumentIO:  #(threading.Thread):\n",
    "    \"\"\"s\n",
    "    基于多线程读取写入文件,判断文件来源.\n",
    "    实例化此类实现多线程,外部使用2个进程,每个实例是1个线程,进程内部多线程读取.\n",
    "    \"\"\"\n",
    "    # sql_mark标明两个文件是必须从sqldb中读取，一部分然后文件中读取合并在一起。\n",
    "    # 其他文件都是根据更新时间选择读取来源\n",
    "    sql_mark = [\n",
    "        {'identity': 'mc_daily_sales', 'mode': 'merge'},\n",
    "        {'identity': 'vip_daily_sales', 'mode': 'merge'}\n",
    "    ]\n",
    "    sql_db = sqlite_init.sql_db\n",
    "    files = None\n",
    "#     mutex = threading.Lock()\n",
    "    queue = multiprocessing.Queue()\n",
    "\n",
    "    @classmethod\n",
    "    def get_files_list(cls) -> None:\n",
    "        files = Path(st.DOCS_PATH)\n",
    "        files_list = [{\n",
    "            'identity': None,\n",
    "            'file_name': str(file),\n",
    "            'file_mtime': file.stat().st_mtime,\n",
    "            'file_mtime_in_sqlite': None,\n",
    "            'updated_sqlite': False\n",
    "        } for file in files.glob('*')]\n",
    "        cls.files = files_list\n",
    "\n",
    "    @classmethod\n",
    "    def check_files_list(cls) -> list:\n",
    "        cls.get_files_list()\n",
    "        files = str.join(',', [file['file_name'] for file in cls.files])\n",
    "        for doc in st.DOC_REFERENCE:\n",
    "            existence = re.search(doc['key_words'], files)\n",
    "            if existence is None:\n",
    "                if doc['importance'] == 'required':\n",
    "                    print(f\"缺少必需重要数据表格: {doc['name']}\\n\")\n",
    "                    sys.exit()\n",
    "                elif doc['importance'] == 'caution':\n",
    "                    print(f\"缺少数据表格: {doc['identity']}\\n\")\n",
    "                else:\n",
    "                    pass  # optional文件不存在时不需要提醒\n",
    "            for file in cls.files:\n",
    "                matched = re.search(doc['key_words'], file['file_name'])\n",
    "                if matched is not None:\n",
    "                    file['identity'] = doc['identity']\n",
    "#         cls.mutex.aquaire()\n",
    "        conn = sqlite.connect(cls.sql_db)\n",
    "        cursor = conn.cursor()\n",
    "        cursor_data = cursor.execute(\"SELECT identity, file_name, file_mtime FROM tmj_files_info;\")\n",
    "        # print(files_list)\n",
    "        for row in cursor_data:  # 把查询到的sqlite中的文件更新时间放入files_list中,后续对比会用到\n",
    "            for file in cls.files:\n",
    "                # print(file)\n",
    "                if file['identity'] == row[1]:\n",
    "                    file['file_mtime_in_sqlite'] = row[3]\n",
    "                    # print('修改了')\n",
    "        conn.close()\n",
    "#         cls.mutex.release()\n",
    "        return cls.files\n",
    "\n",
    "    def __init__(self, doc_reference):\n",
    "        super().__init__()\n",
    "        self.identity = doc_reference['identity']\n",
    "        self.doc_ref = doc_reference\n",
    "        self.file = None\n",
    "        self.from_sql = None\n",
    "        self.queue = self.queue\n",
    "#         self.mutex = self.mutex\n",
    "        if DocumentIO.files is None:\n",
    "            self.files = self.check_files_list()\n",
    "        self.check_file()\n",
    "\n",
    "    def check_file(self) -> None:\n",
    "        file_name = []\n",
    "        for file in self.files:\n",
    "            if file['identity'] == self.identity:\n",
    "                file_name.append(file['file_name'])\n",
    "                if file['file_mtime'] == file['file_mtime_in_sqlite']:\n",
    "                    self.from_sql = 'substitute'\n",
    "        if len(file_name) > 0:\n",
    "            self.file = file_name\n",
    "        for doc in DocumentIO.sql_mark:\n",
    "            if self.identity == doc['identity']:\n",
    "                self.from_sql = doc['mode']\n",
    "\n",
    "    def read_doc(self) -> pd.DataFrame():\n",
    "        doc_df = pd.DataFrame()\n",
    "        for file in self.file:\n",
    "            matched_csv = re.match(r'^.*\\.csv$', file)\n",
    "            matched_excel = re.match(r'^.*\\.xlsx?$', file)\n",
    "            pd_cols = self.doc_ref['key_pos'].extend(self.doc_ref['val_pos'])\n",
    "            print('pd_cols:',pd_cols)\n",
    "            if matched_csv:\n",
    "                df = pd.read_csv(file, usecols=lambda col: col in pd_cols)\n",
    "                doc_df = pd.concat([doc_df, df], axis=0)\n",
    "            if matched_excel:\n",
    "                df = pd.read_excel(file, usecols=lambda col: col in pd_cols)  # 在read_excel中使用index_col=[]报错,不知道原因\n",
    "                doc_df = pd.concat([doc_df, df], axis=0)\n",
    "        return doc_df\n",
    "\n",
    "    def read_sqlite(self) -> pd.DataFrame():\n",
    "        pd_cols = self.doc_ref['key_pos'].extend(self.doc_ref['val_pos'])\n",
    "        sql_constraint = ''\n",
    "        if self.from_sql == 'merge':\n",
    "            sales_date_head = datetime.datetime.today() - datetime.timedelta(days=st.VIP_SALES_INTERVAL)\n",
    "            sql_constraint = f' WHERE 日期 >= {sales_date_head}'\n",
    "#         self.mutex.aquaire()\n",
    "        conn = sqlite.connect(self.sql_db)\n",
    "        # sql_cursor = conn.cursor()\n",
    "        sql_query = f\"SELECT {str.join(',', pd_cols)}, FROM {self.identity}{sql_constraint}\"\n",
    "        sql_df = pd.read_sql_query(sql_query, con=conn, index_col=self.doc_ref['key_pos'])\n",
    "        conn.close()\n",
    "#         self.mutex.release()\n",
    "        return sql_df\n",
    "\n",
    "    def get_data(self) -> pd.DataFrame():\n",
    "        if self.from_sql == 'merge':\n",
    "            doc_df = self.read_doc()\n",
    "            sql_df = self.read_sqlite()\n",
    "            sql_date = pd.DataFrame()\n",
    "            if not doc_df.empty:\n",
    "                doc_df['日期'] = pd.to_datetime(doc_df['日期'])\n",
    "                # doc_date = doc_df.drop_duplicates(subset=['日期'], keep='first')['日期']\n",
    "            if not sql_df.empty:\n",
    "                sql_df['日期'] = pd.to_datetime(sql_df['日期'])\n",
    "                sql_date = sql_df.drop_duplicates(subset=['日期'], keep='first')['日期']\n",
    "            if not (doc_df.empty or sql_df.empty):\n",
    "                mask = [False if x in sql_date else True for x in doc_df['日期']]\n",
    "                merged_df = pd.concat([doc_df[mask], sql_df], keys=['doc_df', 'sql_df'])\n",
    "            else:\n",
    "                merged_df = pd.concat([doc_df, sql_df], keys=['doc_df', 'sql_df'])\n",
    "            return merged_df\n",
    "        elif self.from_sql == 'substitute':\n",
    "            sql_df = self.read_sqlite()\n",
    "            return sql_df\n",
    "        else:\n",
    "            doc_df = self.read_doc()\n",
    "            return doc_df\n",
    "\n",
    "        pass\n",
    "\n",
    "    def to_sqlite(self):\n",
    "#         self.mutex.aquaire()\n",
    "        conn = sqlite.connect(self.sql_db)\n",
    "        cursor = conn.cursor()\n",
    "        pass\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        query_data = []\n",
    "        count = 1\n",
    "        for file in self.files:\n",
    "            if file['identity'] is not None:\n",
    "                query_data.append((count, file['identity'], file['file_name'], file['file_mtime']))\n",
    "                count += 1\n",
    "        # 把最新的文件信息写进sqlite中,用于下一次比对,旧信息全部删除.\n",
    "        print(self.files)\n",
    "        cursor.execute(\"DELETE FROM tmj_files_info;\")\n",
    "        cursor.executemany(\n",
    "            \"INSERT INTO tmj_files_info(id, identity, file_name, file_mtime) VALUES(?,?,?,?);\", query_data)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        \n",
    "doc_rf = {\n",
    "        'identity': 'vip_daily_sales', 'name': '',  # 日销量、商品链接\n",
    "        'key_words': '商品明细|条码粒度', 'key_pos': ['条码', '日期', ], 'val_pos': ['销售量', '商品链接', ],\n",
    "        'val_type': ['INT', 'TEXT', ],\n",
    "        'importance': 'caution'\n",
    "    }\n",
    "get_raw_data = DocumentIO(doc_rf)\n",
    "df = get_raw_data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd11d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 461, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 450, in process_one\n",
      "    await dispatch(*args)\n",
      "TypeError: object NoneType can't be used in 'await' expression\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python38\\lib\\logging\\__init__.py\", line 1085, in emit\n",
      "    self.flush()\n",
      "  File \"c:\\python38\\lib\\logging\\__init__.py\", line 1065, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 22] Invalid argument\n",
      "Call stack:\n",
      "  File \"c:\\python38\\lib\\runpy.py\", line 192, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\python38\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\python38\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\python38\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\python38\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\python38\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\python38\\lib\\asyncio\\base_events.py\", line 563, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\python38\\lib\\asyncio\\base_events.py\", line 1844, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\python38\\lib\\asyncio\\events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 463, in dispatch_queue\n",
      "    self.log.exception(\"Error in message handler\")\n",
      "Message: 'Error in message handler'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c43bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
